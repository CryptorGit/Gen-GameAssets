# SAM3Dベース「3Dアセット工場」構想ドキュメント（現時点のやりたいこと整理）

## 1. 背景と目的

### 背景
- Meta の SAM3D Playground で得られる **1枚画像からの高品質3D再構成** が、現状他モデルより頭ひとつ抜けている。
- あなたの目的は「一回遊んで終わり」ではなく、  
  **ゲーム用の肉・串・パーツなどの3Dアセットを量産する工場を自分で持つこと**。
- Colab 上で SAM3D OSS を無理やり動かそうとしたが、
  - 依存関係（auto-gptq / kaolin / gsplat / pytorch3d 等）が重すぎる
  - 環境が揮発的で、何度もセットアップを繰り返す羽目になる
  という理由で、時間対効果が悪いと判断。

### 目的
- **Meta SAM3D の品質を維持しつつ、自前のインフラ上で「画像→3D」を回せる仕組みを作る。**
- 最終的には、あなたのゲーム（串焼き・たき火ゲームなど）のアセット生成パイプラインに組み込み、
  - スプライト画像を用意
  - マスクを用意（もしくは SAM 系で自動生成）
  - バッチで SAM3D を叩き、3Dアセットを量産  
  できる状態まで持っていく。

---

## 2. やりたいこと（機能レベル）

### 2.1 コア機能（必須）

1. **画像＋マスクからの 3D 生成 API**
   - 入力：
     - RGB画像（例：肉のスプライト）
     - その画像に対応するマスク画像（対象オブジェクト領域）
   - 処理：
     - SAM3D の Inference パイプラインを呼び出し、3D表現を生成
   - 出力：
     - 最低限：点群 / Gaussian Splatting 形式（.ply 等）
     - 余裕があれば：メッシュ＋テクスチャ（.glb / .gltf）

2. **バッチ処理**
   - 複数の画像＋マスクを一括投入し、順番に 3D 化する。
   - 将来的には、ゲーム内の「素材リスト」から定期的に 3D アセットを生成する運用を想定。

3. **自前インフラ上での再現性**
   - 一度コンテナを組んでしまえば、
     - 環境が壊れない
     - 別の日に同じ Docker イメージで再実行可能
   - Modal.com 上の GPU コンテナとしてホストし、必要なときに起動して叩ける。

### 2.2 UI 周りでやりたいこと

1. **SAM3D Playground 風の Web UI（将来のゴール）**
   - Meta 公式 Playground と同等のフロー：
     1. 画像アップロード
     2. インタラクティブなマスク選択（ポイントクリック / 範囲選択）
     3. 「3D生成」ボタン
     4. 結果プレビュー（簡易的なビューで回して見る）
     5. ダウンロード（PLY / GLB 等）
   - 完全なUIコピーではなく、「ワークフローと体験」を真似る。

2. **簡易バッチUI**
   - 手元でまとめて素材画像を指定して、バックエンドに投げる簡易ツール（Web or CLI）。
   - 人間の手作業を減らし、「素材を並べておけば勝手に3D化される」状態に寄せる。

---

## 3. 技術スタックの方針

### 3.1 バックエンド（推論基盤）

- **基盤：Modal.com**
  - GPU付き関数をホストし、HTTP的に叩ける環境として利用。
  - `Image.from_dockerfile()` を使い、**自前の Dockerfile で SAM3D 推論環境を定義**する。

- **コンテナ：Docker + NVIDIA CUDA ベース**
  - 例：`nvcr.io/nvidia/pytorch:24.xx-py3` などの PyTorch付きCUDAイメージをベースにする。
  - `sam-3d-objects` リポジトリをコンテナにコピーし、`requirements.inference.txt` ベースで依存をインストール。
  - 問題児（auto-gptq, kaolin, 開発ツール群）は最初から外す／必要なところだけパッチで殺す。

- **実行パス：**
  - 最初の段階では `demo.py` を Modal 内で動かせるかが第一マイルストーン。
  - その後、`Inference` クラスを直接呼び出す専用エントリポイントを作る。

### 3.2 フロントエンド（将来的）

- **候補：Next.js + React + Tailwind CSS**
  - SAM / SAM2 WebUI で一般的な構成を踏襲。
  - 最初はローカルホスト上のシンプルな UI で十分。
- **短期ゴール：**
  - いきなり Playground 完全クローンは狙わず、
    - 画像アップロード
    - マスク画像アップロード（or 手動パス指定）
    - 「生成」ボタン
    - 出力ファイル名一覧とダウンロードリンク  
    程度のシンプルな管理画面から始める。

---

## 4. スコープ（含めるもの / 含めないもの）

### 4.1 当面スコープに含めるもの

- SAM3D 推論をコンテナ化して Modal 上で動かすこと。
- 画像＋マスク→PLY（もしくは同等の3D表現）の一方向変換。
- 1枚〜少数枚を対象とするテストパイプライン（肉など、具体的なゲーム用素材で試す）。
- 簡易的な CLI or Python スクリプトでの利用：
  - ローカルの画像パスを渡す
  - Modal 関数を呼び出して結果をローカルに保存

### 4.2 当面スコープ外（後回し）

- 完全な SAM3D Playground UI のクローン。
- 高品質テクスチャ付き GLB を大量生成する超本気構成：
  - `nvdiffrast` の導入
  - マルチカメラレンダリング調整
  - 高解像度メッシュの最適化
- ゲームエンジン（Unity/Unreal）側でのインポート・シェーダ調整・最適化の自動化。
- 高負荷バッチ処理（数百〜数千枚レベル）の常時運用。

---

## 5. 現状の課題認識

- SAM3D OSS の依存が重く、**Colab のような揮発環境では毎回セットアップが崩壊する**。
- Kaolin や auto-gptq など、「推論本体には不要な依存」に時間を食われている。
- あなたの時間は「環境バグ潰し」よりも、  
  **ゲーム側のデザイン・仕様策定・パイプライン全体設計**に使うべき。
- したがって：
  - 一回ちゃんとコンテナで環境を固定し、
  - Modal に載せて「1 API」として扱う  
  という方向に舵を切る必要がある。

---

## 6. 直近のマイルストーン

1. **ローカルで Modal プロジェクトを作る**
   - `sam3d_modal/` ディレクトリ作成
   - `sam-3d-objects` を clone
   - `Dockerfile` と `sam3d_modal_app.py` のひな型を置く

2. **Modal 上で demo を一度通す**
   - `modal run sam3d_modal_app.py::run_demo` が  
     エラー無しで完走する状態を作る。
   - この時点で、「コンテナ＋Modal＋GPU」の組み合わせが初めて意味を持つ。

3. **画像・マスクを引数で受け取り、PLY を返す関数を作る**
   - `run_sam3d(image_bytes, mask_bytes) -> ply_bytes` の形に整理。
   - ローカルの Python スクリプトから呼び出し、実際に肉画像を 3D 化してみる。

4. **簡易UI or CLI でバッチ投入**
   - とりあえず CLI ベースで、
     - `images/` ディレクトリ以下のペア画像を列挙
     - Modal 関数に順番に投げる
   - 結果の PLY を `outputs/` に保存。

5. （余裕が出たら）**Playground 風の簡易フロント**
   - Next.js or シンプルな React SPA から Modal の API を叩く。

---

## 7. 最終イメージ

最終的にあなたが持ちたいものは：

- **「画像とマスクさえ投げれば、裏でSAM3Dが回って、3Dアセットが吐き出される工場」**
- それが
  - Modal 上の GPU 関数として安定稼働し、
  - コンテナで環境が固定され、
  - ゲームのアセットパイプラインに統合されている状態。

いまやっているのは、そのための**基礎工事**です。  
Colab で環境崩壊と殴り合うフェーズは切り上げて、  
コンテナ＋Modal に全振りする、というのがこのドキュメントの整理した方針です。
